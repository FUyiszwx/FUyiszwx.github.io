
## 1.编码器的设计原理

* `双向上下文处理` - 通过自注意力机制，考虑输入序列每个元素之前和之后的上下文信息，使得输入元素在编码的同时考虑到整个序列的所有元素。
* `层级结构` - 编码器每一层中包含SelfAttention和FeedForward。逐层深入地处理信息，在前一层的基础上细化和增强输入序列的表示。
* `残差连接和层归一化` - 避免深层网络训练中的梯度爆炸和梯度消失

## 2.编码器的输入

   输入主要为预处理后的序列数据。包括以下几个方面：

* `（1）输入序列` - NLP任务中，输入序列由句子组成。首先将文本数据转换成一系列标记（tokens）。这个过程称为标记化。目的是将文本切割为更小的单元，以便进一步的处理和分析。
    * `空格分割标记化` : 直接用空格切割文本划为单词。适用于英语
    * `规则标记化` : 利用正则表达式处理文本的复杂模式，比如缩写、数字和标点。
    * `子词标记化` : 利用词根、词缀分解单词。
    * `字节对编码（BPE）` : BPE是一种流行的字词标记化方法，最初用于数据压缩，后来广泛应用于NLP中处理未知词和减少词汇表大小，通过迭代合并频繁出现的字节对，来学习常见子词。
    例如：
    输入："lower newer mower"
    学习：合并"er"和"new"频繁序列
    * `WordPiece` : 类似BPE，优化语言模型的似然，而不仅仅是序列频率。
    例如：
    输入："JetBrains creates tools for developers."
    标记结果: ["Jet", "##B", "##rains", "creates", "tools", "for", "developers", "."]

* `（2）词嵌入（Embeddings）` - 输入序列中的每个标记被转换成一个固定维度的向量，通过词嵌入实现。词嵌入通过学习的方法获得，将词汇中的语义信息编码成高维空间的点。
* `（3）位置编码（Positional Encodings）` -
* `（4）组合词嵌入和位置编码` - 送入编码器的自注意力层之前，每个标记的词嵌入向量和其位置编码向量相结合，生成一个综合的表示。这个表示蕴含了词汇的语义内容，也包括了该词在输入序列中的位置信息。通过这种方式，编码器在执行自注意力计算时能同时考虑到每个词的语义和它在序列中的位置，无论是相对位置还是绝对位置。
* `（5）处理流程` - 输入数据进行词嵌入，结合位置编码。进入自注意力层加工，捕获输入中的复杂模式和依赖关系。



## 3.编码器的输出
    输出为一个深度处理过的序列表示，具体特点如下：

* `（1）高度提炼的特征表示` - 输出是一组经过多层变换的向量，每个向量代表输入序列中的相应位置的复杂特征表示。
* `（2）高上下文感知的嵌入` - 自注意力机制的特性，编码器输出的每个向量不仅编码了对应位置的单词信息，还整合了整个输入序列的上下文信息。这意味着每个输出向量都反映了其他位置的信息，使得每个位置的表示都具有全局视角。
* `（3）维度保持` - 输出向量通常保持和输入嵌入相同的维度。这意味着整个编码过程不改变数据的形状，优点如下：
    * `维持信息完整性` - 信息不会因为维度缩减丢失。
    * `简化模型结构` - 每一层输入无缝作为输出。
    * `方便残差连接` - 残差连接要求输入和输出维度相同，这样可以直接将输入加到输出上，有助于解决深层网络中的梯度消失。


