## 1.残差连接的原理

残差连接旨在帮助深层模型更好训练的技巧。之所以称为残差，
是因为它允许模型在学习过程中直接学习到输入与输出之间的差异，
也就是残差。核心思想在于，网络学习到的不是完整的输出，而是和
输入的差异。

* `常规层连接`
    传统的神经网络层之间的连接方式中，每一层直接作为下一层的输入：

    $$ X^{(i)} = Layer(X^{(i-1)}) $$

    其中，$X^{(i)}$表示第i层的输出，$X^{(i-1)}$表示第i-1层的输出，直接传递给第i层的函数Layer。

* `残差连接的引入`
    在引入残差连接的网络中，每一层的输出不仅仅是这一层直接处理的结果，而是这一层的处理结果加上输入本身：
    
    $$ X^{(i)} = X^{(i-1)} + Layer(X^{(i-1)}) $$

    这种方式可以帮助梯度在网络中更有效地流动，因为在反向传播中梯度可以直接通过加法操作传递回更早的层。

## 2.残差连接的优点

* `改善梯度流动`：深层网络中，直接凑够输出到输入传播梯度，可能会出现梯度消失或者梯度爆炸。残差连接允许梯度直接跨过多个层传播，保持了梯度的稳定性。
  
* `易于学习的身份映射`：理想情况中，如果某一层不需要改变，最优的函数可能是输出等于输入的映射。残差连接使得网络轻易地学习这种映射，也就是$Layer(X^{i-1})$为0。

* `梯度优化`：残差连接中，理论上连接点的梯度可以是1，意味着原始的输入可以不经任何衰减地传递至网络的更深层，这对于深层网络训练非常有利。

    $$ \frac{\partial{X^{(i)}}}{\partial{X^{(i-1)}}} = 1 + \frac{\partial{Layer(X^{(i-1)})}}{\partial{X^{(i-1)}}} $$

    公式中的1，表示原始输入可以无衰减的传递至更深层，有利于深层网络训练。


* `偏向恒等函数`：残差连接使得网络偏向于学习接近[恒等映射]的功能，意味着如果输入与输出的最优映射接近恒等映射，网络能够更容易地学习这种映射。在实际应用中被证明是非常有效的，在许多情况下，网络的目标就是在保留输入信息的同时学习输入与输出之间的细微差异。